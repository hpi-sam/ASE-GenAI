{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/answerList_data.csv')\n",
    "data_tp = data[data['TP'] == True]\n",
    "data_tp_group_by_method = data_tp.groupby('FailingMethod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write explanations to file\n",
    "for method, group in data_tp_group_by_method:\n",
    "    for i, (index, row) in enumerate(group.iterrows(), 1):\n",
    "        os.makedirs('data/correct_explainations', exist_ok=True)\n",
    "        with open(f'data/correct_explainations/{method}.txt', 'a') as f:\n",
    "            f.write(f\"{i}: {row['Answer.explanation']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_explainations = {}\n",
    "\n",
    "for method, group in data_tp_group_by_method:\n",
    "    correct_explainations[method] = []\n",
    "    for i, (index, row) in enumerate(group.iterrows(), 1):\n",
    "        correct_explainations[method].append(row['Answer.explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_consolidated_explanations = {}\n",
    "dir = 'data/llm_consolidated_explanations'\n",
    "variant = ['1', '2', '3', '4']\n",
    "\n",
    "\n",
    "for variant_name in variant:\n",
    "    llm_consolidated_explanations[variant_name] = {}  # Initialize dict for each variant\n",
    "    for filename in os.listdir(f'{dir}/{variant_name}'):\n",
    "        if filename.endswith('.txt'):\n",
    "            method = filename[:-4]  # Remove .txt extension\n",
    "            with open(f'{dir}/{variant_name}/{filename}', 'r') as f:\n",
    "                llm_consolidated_explanations[variant_name][method] = f.read().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantinspiess/Developer/HPI/ASE/MiniProject 2/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/konstantinspiess/Developer/HPI/ASE/MiniProject 2/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score for each method\n",
    "\n",
    "weights = (0.5, 0.5, 0, 0)\n",
    "bleu_scores = {}\n",
    "\n",
    "for variant_name in variant:\n",
    "    bleu_scores[variant_name] = {}\n",
    "    for method in llm_consolidated_explanations[variant_name]:\n",
    "        references = []\n",
    "        for explanation in correct_explainations[method]:\n",
    "            references.append(explanation.lower().split(\" \"))\n",
    "        candidate = llm_consolidated_explanations[variant_name][method].lower().split(\" \")\n",
    "        score = sentence_bleu(references, candidate, weights=weights)\n",
    "        bleu_scores[variant_name][method] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE score for each method\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = {}\n",
    "\n",
    "for variant_name in variant:\n",
    "    rouge_scores[variant_name] = {}\n",
    "    for method in llm_consolidated_explanations[variant_name]:\n",
    "        scores = {key: [] for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "        candidate = llm_consolidated_explanations[variant_name][method].lower()\n",
    "        \n",
    "        for reference in correct_explainations[method]:\n",
    "            temp_scores = scorer.score(reference.lower(), candidate)\n",
    "            for key in temp_scores:\n",
    "                scores[key].append(temp_scores[key])\n",
    "        \n",
    "        rouge_scores[variant_name][method] = {}\n",
    "        for key in scores:\n",
    "            avg_score = sum(score.fmeasure for score in scores[key]) / len(scores[key])\n",
    "            rouge_scores[variant_name][method][key] = avg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method      Variant   BLEU      ROUGE1    ROUGE2    ROUGEL    \n",
      "HIT01_8     1         0.5592      0.2584      0.0425      0.1754\n",
      "\n",
      "HIT02_24    1         0.2887      0.2394      0.0265      0.1526\n",
      "\n",
      "HIT03_6     2         0.4436      0.1796      0.0355      0.1143\n",
      "\n",
      "HIT04_7     3         0.2887      0.1467      0.0188      0.0887\n",
      "\n",
      "HIT05_35    3         0.3717      0.2052      0.0306      0.1402\n",
      "\n",
      "HIT06_51    3         0.3354      0.1891      0.0368      0.1370\n",
      "\n",
      "HIT07_33    4         0.4488      0.2361      0.0698      0.1701\n",
      "\n",
      "HIT08_54    4         0.5707      0.2239      0.0617      0.1367\n"
     ]
    }
   ],
   "source": [
    "# Print scores as a table\n",
    "print(f\"{'Method':<12}{'Variant':<10}{'BLEU':<10}{'ROUGE1':<10}{'ROUGE2':<10}{'ROUGEL':<10}\")\n",
    "\n",
    "# Get all unique methods\n",
    "all_methods = set()\n",
    "for variant_name in variant:\n",
    "    all_methods.update(llm_consolidated_explanations[variant_name].keys())\n",
    "\n",
    "prev_method = None\n",
    "for method in sorted(all_methods):\n",
    "    if prev_method is not None:\n",
    "        print() # Add blank line between methods\n",
    "    for variant_name in variant:\n",
    "        if method in llm_consolidated_explanations[variant_name]:\n",
    "            print(f\"{method:<12}{variant_name:<10}{bleu_scores[variant_name][method]:.4f}{' ':<6}{rouge_scores[variant_name][method]['rouge1']:.4f}{' ':<6}{rouge_scores[variant_name][method]['rouge2']:.4f}{' ':<6}{rouge_scores[variant_name][method]['rougeL']:.4f}\")\n",
    "    prev_method = method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU scores:\n",
      "  1: 0.4239\n",
      "  2: 0.4436\n",
      "  3: 0.3319\n",
      "  4: 0.5097\n",
      "\n",
      "Average ROUGE-1 scores:\n",
      "  1: 0.2489\n",
      "  2: 0.1796\n",
      "  3: 0.1803\n",
      "  4: 0.2300\n",
      "\n",
      "Average ROUGE-2 scores:\n",
      "  1: 0.0345\n",
      "  2: 0.0355\n",
      "  3: 0.0288\n",
      "  4: 0.0658\n",
      "\n",
      "Average ROUGE-L scores:\n",
      "  1: 0.1640\n",
      "  2: 0.1143\n",
      "  3: 0.1220\n",
      "  4: 0.1534\n"
     ]
    }
   ],
   "source": [
    "# Calculate average BLEU scores for each variant\n",
    "print(\"Average BLEU scores:\")\n",
    "for variant_name in variant:\n",
    "    avg_bleu = sum(bleu_scores[variant_name].values()) / len(bleu_scores[variant_name])\n",
    "    print(f\"  {variant_name}: {avg_bleu:.4f}\")\n",
    "\n",
    "print(\"\\nAverage ROUGE-1 scores:\")\n",
    "for variant_name in variant:\n",
    "    rouge1_scores = [scores['rouge1'] for scores in rouge_scores[variant_name].values()]\n",
    "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "    print(f\"  {variant_name}: {avg_rouge1:.4f}\")\n",
    "\n",
    "print(\"\\nAverage ROUGE-2 scores:\")\n",
    "for variant_name in variant:\n",
    "    rouge2_scores = [scores['rouge2'] for scores in rouge_scores[variant_name].values()]\n",
    "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "    print(f\"  {variant_name}: {avg_rouge2:.4f}\")\n",
    "\n",
    "print(\"\\nAverage ROUGE-L scores:\")\n",
    "for variant_name in variant:\n",
    "    rougeL_scores = [scores['rougeL'] for scores in rouge_scores[variant_name].values()]\n",
    "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "    print(f\"  {variant_name}: {avg_rougeL:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
