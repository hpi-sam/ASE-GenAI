{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from pandas.core.interchange.dataframe_protocol import DataFrame\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, LabelEncoder\n",
    "\n",
    "from xgboost import XGBClassifier  #Does not work with sklearn version>=1.6"
   ],
   "id": "d708b870252fb8f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the data set\n",
    "train_set = pd.read_csv(\"../../data/answerList_data.csv\")\n",
    "file_path = \"../../data/\"\n",
    "# iterate over all files in the directory and store the content\n",
    "files = {}\n",
    "for filename in os.listdir(file_path):\n",
    "    if filename.startswith(\"HIT\"):\n",
    "        # file is a java file read the file content and store it in the dictionary\n",
    "        files[filename.split(\".\")[0]] = open(file_path + filename, \"r\").read()"
   ],
   "id": "6c01a2283586a6d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_set.head(50)",
   "id": "a699d7d7e71881f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# shuffle the data set\n",
    "train_set = train_set.sample(frac=1, random_state=30).reset_index(drop=True)\n",
    "\n",
    "# replace \"Answer.option\" with \"Answer.option\"\n",
    "replace_dict = {\"NO\": 0, \"YES\": 1, \"IDK\": 2}\n",
    "train_set[\"Answer.option\"] = train_set[\"Answer.option\"].replace(replace_dict)"
   ],
   "id": "a056efd91a5ea6ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use textstat to calculate the Flesch reading ease score for the explanation column keep the original column\n",
    "train_set[\"Answer.explanation_length\"] = train_set[\"Answer.explanation\"].apply(\n",
    "    lambda x: len(str(x)) if pd.notnull(x) else None)\n",
    "train_set[\"Flesch_reading_ease\"] = train_set[\"Answer.explanation\"].apply(\n",
    "    lambda x: textstat.flesch_reading_ease(x) if pd.notnull(x) else None)\n",
    "\n",
    "# drop answer.explanation column\n",
    "train_set = train_set.drop(labels=[\"Answer.explanation\"], axis=1)"
   ],
   "id": "3ba6bd85a9006176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# remove unnecessary columns\n",
    "# and apply the StandardScaler to scale them. Replace the original numerical columns\n",
    "train_set = train_set.drop(labels=[\"Answer.ID\", \"Question.ID\", \"FP\", \"FN\", \"TP\", \"TN\", \"Worker.ID\"], axis=1)\n"
   ],
   "id": "c6ff0ee0f6e08408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# use standard scaler for all numerical columns\n",
    "# Identify numerical columns and use StandardScaler to scale them\n",
    "numerical_cols = train_set.select_dtypes(include=['float64', 'int64']).columns\n",
    "# Remove the GroundTruth, Answer.explanation_length and Flesch_reading_ease  columns from the list of numerical columns\n",
    "numerical_cols = numerical_cols.drop(\"Answer.explanation_length\")\n",
    "numerical_cols = numerical_cols.drop(\"Flesch_reading_ease\")\n",
    "numerical_cols = numerical_cols.drop(\"GroundTruth\")\n",
    "numerical_cols = numerical_cols.drop(\"Answer.option\")\n",
    "scaler = StandardScaler()\n",
    "train_set[numerical_cols] = scaler.fit_transform(train_set[numerical_cols])\n"
   ],
   "id": "95950c6fc793e78e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Use label encoding to transform the categorical columns into numerical columns and replace the original columns, do not replace multi_cols\n",
    "multi_cols = [\"Worker.whereLearnedToCode\", \"Worker.programmingLanguage\"]\n",
    "label_encoder = LabelEncoder()\n",
    "mapping_dict = {}\n",
    "for column in train_set.columns:\n",
    "    if column not in multi_cols and train_set[column].dtype == \"object\":\n",
    "        # for FailingMethod and Worker.profession store the original values in another dictionary\n",
    "        if column == \"FailingMethod\" or column == \"Worker.profession\":\n",
    "            unique_cols = train_set[column].unique()\n",
    "            mapping_dict[column] = {v: k for k, v in enumerate(unique_cols)}\n",
    "            train_set[column] = label_encoder.fit_transform(train_set[column].astype(str))\n",
    "        train_set[column] = label_encoder.fit_transform(train_set[column].astype(str))\n",
    "\n"
   ],
   "id": "89a827a7d1d02b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the multilabel column into a list of labels\n",
    "multi_cols = [\"Worker.whereLearnedToCode\", \"Worker.programmingLanguage\"]\n",
    "for column in multi_cols:\n",
    "    # set column to lower case\n",
    "    train_set[column] = train_set[column].str.lower()\n",
    "    # split the column by ;\n",
    "    train_set[column] = train_set[column].apply(lambda x: x.split(\";\") if pd.notnull(x) else None)\n",
    "    # remove the spaces from the list\n",
    "    for i in range(len(train_set[column])):\n",
    "        if train_set[column][i] is not None:\n",
    "            train_set[column][i] = [x.strip() for x in train_set[column][i]]\n",
    "\n",
    "    # replace None with empty list\n",
    "    train_set[column] = train_set[column].apply(lambda x: [] if x is None else x)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Transform the multi-selection column into a one-hot encoded DataFrame\n",
    "    one_hot_encoded = pd.DataFrame(mlb.fit_transform(train_set[column]),\n",
    "                                   columns=mlb.classes_,\n",
    "                                   index=train_set.index)\n",
    "\n",
    "    # Merge the one-hot encoded columns back with the original DataFrame\n",
    "    train_set = pd.concat([train_set, one_hot_encoded], axis=1)\n",
    "    train_set.drop(columns=[column], inplace=True)\n",
    "\n"
   ],
   "id": "94fbdc7d4a6a1dce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_set.head(20)",
   "id": "75818f1aaf8f59b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "non_student_set = train_set[~train_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Undergraduate_Student\"]] + [mapping_dict[\"Worker.profession\"][\"Graduate_Student\"]])]\n",
    "\n",
    "professional_set = train_set[train_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Professional_Developer\"]])]\n",
    "\n",
    "hobbyist_set = train_set[train_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Hobbyist\"]] + [mapping_dict[\"Worker.profession\"][\"Other\"]])]\n",
    "\n",
    "student_set = train_set[train_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Undergraduate_Student\"]] + [mapping_dict[\"Worker.profession\"][\"Graduate_Student\"]])]\n",
    "\n",
    "addition_set = non_student_set\n",
    "\n",
    "\n",
    "# split student_set into a training and holdout set\n",
    "holdout_set = student_set.sample(frac=0.2, random_state=30)\n",
    "train_set_students = student_set.drop(holdout_set.index)\n",
    "Y = train_set_students[\"GroundTruth\"] == train_set_students[\"Answer.option\"]\n",
    "X = train_set_students.drop(labels=[\"GroundTruth\"], axis=1)\n",
    "xgb_model = XGBClassifier(random_state=30, eta=0.5, max_depth=200, subsample=0.5, n_estimators=500)\n",
    "xgb_model.fit(X, Y)\n",
    "\n",
    "# Basic K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate different metrics\n",
    "precision_scores = cross_val_score(xgb_model, X, Y, cv=kfold, scoring='precision')\n",
    "recall_scores = cross_val_score(xgb_model, X, Y, cv=kfold, scoring='recall')\n",
    "f1_scores = cross_val_score(xgb_model, X, Y, cv=kfold, scoring='f1')\n",
    "\n",
    "print(\"K-Fold Cross Validation Results:\")\n",
    "print(f\"Precision Scores: {precision_scores}\")\n",
    "print(f\"Average Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std() * 2:.4f})\")\n",
    "print(f\"\\nRecall Scores: {recall_scores}\")\n",
    "print(f\"Average Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std() * 2:.4f})\")\n",
    "print(f\"\\nF1 Scores: {f1_scores}\")\n",
    "print(f\"Average F1: {f1_scores.mean():.4f} (+/- {f1_scores.std() * 2:.4f})\")\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Number of Non-Students\",\"Precision_Holdout\", \"Recall_Holdout\", \"F1_Holdout\", \"MCC_Holdout\"])\n",
    "n = 0\n",
    "while n <= len(addition_set):\n",
    "    # add non_student_set to the holdout set\n",
    "    holdout_set_loop = pd.concat([holdout_set, addition_set.sample(n, random_state=30)])\n",
    "\n",
    "    # Prepare holdout set\n",
    "    X_holdout = holdout_set_loop.drop(labels=[\"GroundTruth\"], axis=1)\n",
    "    y_holdout = holdout_set_loop[\"GroundTruth\"] == holdout_set_loop[\"Answer.option\"]\n",
    "\n",
    "    # Make predictions on holdout set\n",
    "    y_pred_holdout = xgb_model.predict(X_holdout)\n",
    "    holdout_set_loop[\"Predictions\"] = y_pred_holdout\n",
    "\n",
    "    # Calculate recall and precision for each failing method\n",
    "    y_true = holdout_set_loop[\"GroundTruth\"] == holdout_set_loop[\"Answer.option\"]\n",
    "    y_pred = holdout_set_loop[\"Predictions\"]\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # add the results to the dataframe use pd.concat\n",
    "    results = pd.concat([results, pd.DataFrame({\"Number of Non-Students\":n,\"Precision_Holdout\": precision, \"Recall_Holdout\": recall, \"F1_Holdout\": f1, \"MCC_Holdout\": mcc}, index=[0])])\n",
    "\n",
    "    n += 1\n",
    "\n",
    "# store the results in a csv file\n",
    "results.to_csv(\"results.csv\")"
   ],
   "id": "4bd2b4d7c499b923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create t-test for student_set and non_student_set\n",
    "from scipy.stats import ttest_ind\n",
    "print(\"Answer.duration: \", ttest_ind(student_set[\"Answer.duration\"], non_student_set[\"Answer.duration\"]))"
   ],
   "id": "2c4665faa51728a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# create bins\n",
    "complexity_bins = np.linspace(train_set['Flesch_reading_ease'].min(),\n",
    "                              train_set['Flesch_reading_ease'].max(),\n",
    "                              11)\n",
    "\n",
    "complexity_groups = train_set.groupby(pd.cut(train_set['Flesch_reading_ease'], bins=complexity_bins))\n",
    "complexity_correct = complexity_groups.apply(lambda x: (x['GroundTruth'] == x[\"Answer.option\"]).mean() * 100)\n",
    "\n",
    "# create plot\n",
    "bars = ax.bar(range(len(complexity_correct)), complexity_correct, color='lightgreen')\n",
    "ax.set_title('Distribution of Correct Labels by Reading Complexity', pad=20)\n",
    "ax.set_xlabel('Flesch Reading Ease Score', labelpad=10)\n",
    "ax.set_ylabel('Percentage Correct (%)', labelpad=10)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(len(complexity_correct)))\n",
    "ax.set_xticklabels([f'{bin.left:.1f}-{bin.right:.1f}'\n",
    "                    for bin in complexity_correct.index],\n",
    "                   rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 1,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2b71b38848a726af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# create bins\n",
    "length_bins = np.linspace(train_set['Answer.explanation_length'].min(),\n",
    "                          train_set['Answer.explanation_length'].max(),\n",
    "                          11)\n",
    "\n",
    "length_groups = train_set.groupby(pd.cut(train_set['Answer.explanation_length'], bins=length_bins))\n",
    "length_correct = length_groups.apply(lambda x: (x['GroundTruth'] == x[\"Answer.option\"]).mean() * 100)\n",
    "\n",
    "# create plot\n",
    "bars = ax.bar(range(len(length_correct)), length_correct, color='skyblue')\n",
    "ax.set_title('Distribution of Correct Labels by Explanation Length', pad=20)\n",
    "ax.set_xlabel('Explanation Length (characters)', labelpad=10)\n",
    "ax.set_ylabel('Percentage Correct (%)', labelpad=10)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(len(length_correct)))\n",
    "ax.set_xticklabels([f'{int(bin.left)}-{int(bin.right)}'\n",
    "                    for bin in length_correct.index],\n",
    "                   rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 1,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "66bba03847356fb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "xgboost.plot_importance(xgb_model, max_num_features=10)",
   "id": "9b896cdd01459e52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = results\n",
    "\n",
    "# Plot the data with different colors and without markers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"Number of Non-Students\"], df[\"Precision_Holdout\"], color='blue', label=\"Precision\")\n",
    "plt.plot(df[\"Number of Non-Students\"], df[\"Recall_Holdout\"], color='red', label=\"Recall\")\n",
    "plt.plot(df[\"Number of Non-Students\"], df[\"F1_Holdout\"], color='green', label=\"F1 Score\")\n",
    "plt.plot(df[\"Number of Non-Students\"], df[\"MCC_Holdout\"], color='purple', label=\"MCC\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Number of Non-Students\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Non-Students Metric Shifts\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis to start at 0\n",
    "plt.xlim(left=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "id": "27a5a02c5d178ac2"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
