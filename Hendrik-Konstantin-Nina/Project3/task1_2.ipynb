{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, LabelEncoder\n",
    "\n",
    "from xgboost import XGBClassifier  #Does not work with sklearn version>=1.6"
   ],
   "id": "3c6cfca614061282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the data set\n",
    "full_set = pd.read_csv(\"../../data/answerList_data.csv\")\n",
    "file_path = \"../../data/\"\n",
    "# iterate over all files in the directory and store the content\n",
    "files = {}\n",
    "for filename in os.listdir(file_path):\n",
    "    if filename.startswith(\"HIT\"):\n",
    "        # file is a java file read the file content and store it in the dictionary\n",
    "        files[filename.split(\".\")[0]] = open(file_path + filename, \"r\").read()"
   ],
   "id": "b7024ab7186b1d6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "full_set.head(50)",
   "id": "142ea9eb6e730003"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# shuffle the data set\n",
    "full_set = full_set.sample(frac=1, random_state=20).reset_index(drop=True)\n",
    "\n",
    "# replace \"Answer.option\" with \"Answer.option\"\n",
    "replace_dict = {\"NO\": 0, \"YES\": 1, \"IDK\": 2}\n",
    "full_set[\"Answer.option\"] = full_set[\"Answer.option\"].replace(replace_dict)"
   ],
   "id": "b872de5c427dbbcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use textstat to calculate the Flesch reading ease score for the explanation column keep the original column\n",
    "full_set[\"Answer.explanation_length\"] = full_set[\"Answer.explanation\"].apply(\n",
    "    lambda x: len(str(x)) if pd.notnull(x) else None)\n",
    "full_set[\"Flesch_reading_ease\"] = full_set[\"Answer.explanation\"].apply(\n",
    "    lambda x: textstat.flesch_reading_ease(x) if pd.notnull(x) else None)\n",
    "\n",
    "# drop answer.explanation column\n",
    "full_set = full_set.drop(labels=[\"Answer.explanation\"], axis=1)"
   ],
   "id": "d00b4b5a732ab304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# remove unnecessary columns\n",
    "# and apply the StandardScaler to scale them. Replace the original numerical columns\n",
    "full_set = full_set.drop(labels=[\"Answer.ID\", \"Question.ID\", \"FP\", \"FN\", \"TP\", \"TN\", \"Worker.ID\"], axis=1)\n"
   ],
   "id": "a360eddeb3e5cd94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# use standard scaler for all numerical columns\n",
    "# Identify numerical columns and use StandardScaler to scale them\n",
    "numerical_cols = full_set.select_dtypes(include=['float64', 'int64']).columns\n",
    "# Remove the GroundTruth, Answer.explanation_length and Flesch_reading_ease  columns from the list of numerical columns\n",
    "numerical_cols = numerical_cols.drop(\"Answer.explanation_length\")\n",
    "numerical_cols = numerical_cols.drop(\"Flesch_reading_ease\")\n",
    "numerical_cols = numerical_cols.drop(\"GroundTruth\")\n",
    "numerical_cols = numerical_cols.drop(\"Answer.option\")\n",
    "scaler = StandardScaler()\n",
    "full_set[numerical_cols] = scaler.fit_transform(full_set[numerical_cols])\n"
   ],
   "id": "539872856c8f67a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use label encoding to transform the categorical columns into numerical columns and replace the original columns, do not replace multi_cols\n",
    "multi_cols = [\"Worker.whereLearnedToCode\", \"Worker.programmingLanguage\"]\n",
    "label_encoder = LabelEncoder()\n",
    "mapping_dict = {}\n",
    "for column in full_set.columns:\n",
    "    if column not in multi_cols and full_set[column].dtype == \"object\":\n",
    "        # for FailingMethod and Worker.profession store the original values in another dictionary\n",
    "        if column == \"FailingMethod\" or column == \"Worker.profession\":\n",
    "            unique_cols = full_set[column].unique()\n",
    "            mapping_dict[column] = {v: k for k, v in enumerate(unique_cols)}\n",
    "            full_set[column] = label_encoder.fit_transform(full_set[column].astype(str))\n",
    "        full_set[column] = label_encoder.fit_transform(full_set[column].astype(str))\n"
   ],
   "id": "8ba28250e8cbbd65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the multilabel column into a list of labels\n",
    "multi_cols = [\"Worker.whereLearnedToCode\", \"Worker.programmingLanguage\"]\n",
    "for column in multi_cols:\n",
    "    # set column to lower case\n",
    "    full_set[column] = full_set[column].str.lower()\n",
    "    # split the column by ;\n",
    "    full_set[column] = full_set[column].apply(lambda x: x.split(\";\") if pd.notnull(x) else None)\n",
    "    # remove the spaces from the list\n",
    "    for i in range(len(full_set[column])):\n",
    "        if full_set[column][i] is not None:\n",
    "            full_set[column][i] = [x.strip() for x in full_set[column][i]]\n",
    "\n",
    "    # replace None with empty list\n",
    "    full_set[column] = full_set[column].apply(lambda x: [] if x is None else x)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Transform the multi-selection column into a one-hot encoded DataFrame\n",
    "    one_hot_encoded = pd.DataFrame(mlb.fit_transform(full_set[column]),\n",
    "                                   columns=mlb.classes_,\n",
    "                                   index=full_set.index)\n",
    "\n",
    "    # Merge the one-hot encoded columns back with the original DataFrame\n",
    "    full_set = pd.concat([full_set, one_hot_encoded], axis=1)\n",
    "    full_set.drop(columns=[column], inplace=True)\n",
    "\n"
   ],
   "id": "1fbcd00f36e271ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "non_student_set = full_set[~full_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Undergraduate_Student\"]] + [mapping_dict[\"Worker.profession\"][\"Graduate_Student\"]])]\n",
    "\n",
    "professional_set = full_set[full_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Professional_Developer\"]])]\n",
    "\n",
    "hobbyist_set = full_set[full_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Hobbyist\"]] + [mapping_dict[\"Worker.profession\"][\"Other\"]])]\n",
    "\n",
    "student_set = full_set[full_set[\"Worker.profession\"].isin([mapping_dict[\"Worker.profession\"][\"Undergraduate_Student\"]] + [mapping_dict[\"Worker.profession\"][\"Graduate_Student\"]])]\n"
   ],
   "id": "7f1ecf6a1cf90ca1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, matthews_corrcoef\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Training set size\",\"Precision_Holdout\", \"Recall_Holdout\", \"F1_Holdout\", \"MCC_Holdout\"])\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=40, max_depth=1)\n",
    "\n",
    "for n in range(2,len(non_student_set)):\n",
    "    train_set = non_student_set.sample(n=n)\n",
    "    Y = train_set[\"GroundTruth\"] == train_set[\"Answer.option\"]\n",
    "    Y = Y.astype(int)\n",
    "    # ensure that both answer options are present in the training set\n",
    "    if sum(Y) == 0 or sum(Y) == len(Y):\n",
    "        train_set = non_student_set.sample(n=n)\n",
    "        Y = train_set[\"GroundTruth\"] == train_set[\"Answer.option\"]\n",
    "        Y = Y.astype(int)\n",
    "\n",
    "    # holdout set contains all the other samples\n",
    "    holdout_set = full_set[~full_set.index.isin(train_set.index)]\n",
    "\n",
    "    model = xgb_model\n",
    "    X = train_set.drop(labels=[\"GroundTruth\"], axis=1)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # compute the recall, precision, f1 and matthews correlation coefficient\n",
    "    y_true = holdout_set[\"GroundTruth\"] == holdout_set[\"Answer.option\"]\n",
    "    Y_pred = model.predict(holdout_set.drop(labels=[\"GroundTruth\"], axis=1))\n",
    "    recall = recall_score(y_true, Y_pred)\n",
    "    precision = precision_score(y_true, Y_pred)\n",
    "    f1 = f1_score(y_true, Y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, Y_pred)\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame({\"Number of Non-Students\":n,\"Precision_Holdout\": precision, \"Recall_Holdout\": recall, \"F1_Holdout\": f1, \"MCC_Holdout\": mcc}, index=[0])])\n",
    "\n",
    "    if n % 20 == 0:\n",
    "        print(\"Number of non_student_set for training: \" + str(n))\n",
    "    n += 1\n",
    "\n",
    "results.to_csv(\"results1_2.csv\")\n"
   ],
   "id": "9982186d16b15823"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# create bins\n",
    "complexity_bins = np.linspace(train_set['Flesch_reading_ease'].min(),\n",
    "                              train_set['Flesch_reading_ease'].max(),\n",
    "                              11)\n",
    "\n",
    "complexity_groups = train_set.groupby(pd.cut(train_set['Flesch_reading_ease'], bins=complexity_bins))\n",
    "complexity_correct = complexity_groups.apply(lambda x: (x['GroundTruth'] == x[\"Answer.option\"]).mean() * 100)\n",
    "\n",
    "# create plot\n",
    "bars = ax.bar(range(len(complexity_correct)), complexity_correct, color='lightgreen')\n",
    "ax.set_title('Distribution of Correct Labels by Reading Complexity', pad=20)\n",
    "ax.set_xlabel('Flesch Reading Ease Score', labelpad=10)\n",
    "ax.set_ylabel('Percentage Correct (%)', labelpad=10)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(len(complexity_correct)))\n",
    "ax.set_xticklabels([f'{bin.left:.1f}-{bin.right:.1f}'\n",
    "                    for bin in complexity_correct.index],\n",
    "                   rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 1,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5b6c955a9c0b9486"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# create bins\n",
    "length_bins = np.linspace(train_set['Answer.explanation_length'].min(),\n",
    "                          train_set['Answer.explanation_length'].max(),\n",
    "                          11)\n",
    "\n",
    "length_groups = train_set.groupby(pd.cut(train_set['Answer.explanation_length'], bins=length_bins))\n",
    "length_correct = length_groups.apply(lambda x: (x['GroundTruth'] == x[\"Answer.option\"]).mean() * 100)\n",
    "\n",
    "# create plot\n",
    "bars = ax.bar(range(len(length_correct)), length_correct, color='skyblue')\n",
    "ax.set_title('Distribution of Correct Labels by Explanation Length', pad=20)\n",
    "ax.set_xlabel('Explanation Length (characters)', labelpad=10)\n",
    "ax.set_ylabel('Percentage Correct (%)', labelpad=10)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(len(length_correct)))\n",
    "ax.set_xticklabels([f'{int(bin.left)}-{int(bin.right)}'\n",
    "                    for bin in length_correct.index],\n",
    "                   rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 1,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6ba8bb5b5572c3eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "xgboost.plot_importance(xgb_model, max_num_features=10)",
   "id": "9d22d34da6617050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "df = results\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot 1: All metrics with confidence intervals\n",
    "grouped_stats = df.groupby('Number of Non-Students').agg({\n",
    "    'Precision_Holdout': ['mean', 'std'],\n",
    "    'Recall_Holdout': ['mean', 'std'],\n",
    "    'F1_Holdout': ['mean', 'std'],\n",
    "    'MCC_Holdout': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "grouped_stats.columns = ['Number_of_Non_Students',\n",
    "                        'Precision_mean', 'Precision_std',\n",
    "                        'Recall_mean', 'Recall_std',\n",
    "                        'F1_mean', 'F1_std',\n",
    "                        'MCC_mean', 'MCC_std']\n",
    "\n",
    "# Plot means and confidence intervals\n",
    "metrics = [('Precision', 'blue'), ('Recall', 'red'), ('F1', 'green'), ('MCC', 'purple')]\n",
    "\n",
    "for metric, color in metrics:\n",
    "    mean_col = f'{metric}_mean'\n",
    "    std_col = f'{metric}_std'\n",
    "\n",
    "    ax1.plot(grouped_stats['Number_of_Non_Students'],\n",
    "             grouped_stats[mean_col],\n",
    "             label=metric,\n",
    "             color=color,\n",
    "             linewidth=2)\n",
    "\n",
    "    # Add confidence intervals\n",
    "    ax1.fill_between(grouped_stats['Number_of_Non_Students'],\n",
    "                     grouped_stats[mean_col] - grouped_stats[std_col],\n",
    "                     grouped_stats[mean_col] + grouped_stats[std_col],\n",
    "                     color=color,\n",
    "                     alpha=0.1)\n",
    "\n",
    "# Add target lines from 5-fold CV\n",
    "ax1.axhline(y=0.8498, color='blue', linestyle=':', alpha=0.5, label='Target Precision (0.8498)')\n",
    "ax1.axhline(y=0.8806, color='red', linestyle=':', alpha=0.5, label='Target Recall (0.8806)')\n",
    "ax1.axhline(y=0.8647, color='green', linestyle=':', alpha=0.5, label='Target F1 (0.8647)')\n",
    "\n",
    "# Customize plots\n",
    "ax1.set_title('Performance Metrics vs Number of Non-Students in Training Set', pad=20)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('model_non_student_training.png', bbox_inches='tight', dpi=300)\n",
    "plt.close()"
   ],
   "id": "a3b5aa198fdc8e24"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
