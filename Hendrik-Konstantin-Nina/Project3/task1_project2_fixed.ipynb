{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, LabelEncoder\n",
    "\n",
    "from xgboost import XGBClassifier  #Does not work with sklearn version>=1.6"
   ],
   "id": "75b4413cab15f006"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the data set\n",
    "train_set = pd.read_csv(\"../../data/answerList_data.csv\")\n",
    "file_path = \"../../data/\"\n",
    "# iterate over all files in the directory and store the content\n",
    "files = {}\n",
    "for filename in os.listdir(file_path):\n",
    "    if filename.startswith(\"HIT\"):\n",
    "        # file is a java file read the file content and store it in the dictionary\n",
    "        files[filename.split(\".\")[0]] = open(file_path + filename, \"r\").read()"
   ],
   "id": "a5cc3fad46fa2bad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_set.head(50)",
   "id": "5834ddd7b0657eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# shuffle the data set\n",
    "train_set = train_set.sample(frac=1, random_state=20).reset_index(drop=True)\n",
    "\n",
    "# replace \"Answer.option\" with \"Answer.option\"\n",
    "replace_dict = {\"NO\": 0, \"YES\": 1, \"IDK\": 2}\n",
    "train_set[\"Answer.option\"] = train_set[\"Answer.option\"].replace(replace_dict)"
   ],
   "id": "dd116604960b5026"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use textstat to calculate the Flesch reading ease score for the explanation column keep the original column\n",
    "train_set[\"Answer.explanation_length\"] = train_set[\"Answer.explanation\"].apply(\n",
    "    lambda x: len(str(x)) if pd.notnull(x) else None)\n",
    "train_set[\"Flesch_reading_ease\"] = train_set[\"Answer.explanation\"].apply(\n",
    "    lambda x: textstat.flesch_reading_ease(x) if pd.notnull(x) else None)\n",
    "\n",
    "# drop answer.explanation column\n",
    "train_set = train_set.drop(labels=[\"Answer.explanation\"], axis=1)"
   ],
   "id": "29ea201ee9de64a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# remove unnecessary columns\n",
    "# and apply the StandardScaler to scale them. Replace the original numerical columns\n",
    "train_set = train_set.drop(labels=[\"Answer.ID\", \"Question.ID\", \"FP\", \"FN\", \"TP\", \"TN\", \"Worker.ID\"], axis=1)\n"
   ],
   "id": "57fcbeedeeb5dfd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# use standard scaler for all numerical columns\n",
    "# Identify numerical columns and use StandardScaler to scale them\n",
    "numerical_cols = train_set.select_dtypes(include=['float64', 'int64']).columns\n",
    "# Remove the GroundTruth, Answer.explanation_length and Flesch_reading_ease  columns from the list of numerical columns\n",
    "numerical_cols = numerical_cols.drop(\"Answer.explanation_length\")\n",
    "numerical_cols = numerical_cols.drop(\"Flesch_reading_ease\")\n",
    "numerical_cols = numerical_cols.drop(\"GroundTruth\")\n",
    "numerical_cols = numerical_cols.drop(\"Answer.option\")\n",
    "scaler = StandardScaler()\n",
    "train_set[numerical_cols] = scaler.fit_transform(train_set[numerical_cols])\n"
   ],
   "id": "2495a52ffd4621bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Use label encoding to transform the categorical columns into numerical columns and replace the original columns, do not replace multi_cols\n",
    "multi_cols = [\"Worker.whereLearnedToCode\", \"Worker.programmingLanguage\"]\n",
    "label_encoder = LabelEncoder()\n",
    "for column in train_set.columns:\n",
    "    if column not in multi_cols and train_set[column].dtype == \"object\":\n",
    "        # for FailingMethod store the original values in another dictionary\n",
    "        if column == \"FailingMethod\":\n",
    "            failing_methods = train_set[column].unique()\n",
    "            failing_methods_dict = {i: failing_methods[i] for i in range(len(failing_methods))}\n",
    "        train_set[column] = label_encoder.fit_transform(train_set[column].astype(str))\n",
    "\n"
   ],
   "id": "988ccb2fd39f07f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the multilabel column into a list of labels\n",
    "multi_cols = [\"Worker.whereLearnedToCode\", \"Worker.programmingLanguage\"]\n",
    "for column in multi_cols:\n",
    "    # set column to lower case\n",
    "    train_set[column] = train_set[column].str.lower()\n",
    "    # split the column by ;\n",
    "    train_set[column] = train_set[column].apply(lambda x: x.split(\";\") if pd.notnull(x) else None)\n",
    "    # remove the spaces from the list\n",
    "    for i in range(len(train_set[column])):\n",
    "        if train_set[column][i] is not None:\n",
    "            train_set[column][i] = [x.strip() for x in train_set[column][i]]\n",
    "\n",
    "    # replace None with empty list\n",
    "    train_set[column] = train_set[column].apply(lambda x: [] if x is None else x)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Transform the multi-selection column into a one-hot encoded DataFrame\n",
    "    one_hot_encoded = pd.DataFrame(mlb.fit_transform(train_set[column]),\n",
    "                                   columns=mlb.classes_,\n",
    "                                   index=train_set.index)\n",
    "\n",
    "    # Merge the one-hot encoded columns back with the original DataFrame\n",
    "    train_set = pd.concat([train_set, one_hot_encoded], axis=1)\n",
    "    train_set.drop(columns=[column], inplace=True)\n",
    "\n"
   ],
   "id": "c4ef64493b245036"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_set.head(20)",
   "id": "6e9dfeb7c35c0c06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier(random_state=40, max_depth=1)\n",
    "xgb_model.fit(train_set.drop(labels=[\"GroundTruth\"], axis=1), train_set[\"GroundTruth\"] == train_set[\"Answer.option\"])\n"
   ],
   "id": "639508a8a4dbe31c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "model = xgb_model\n",
    "X = train_set.drop(labels=[\"GroundTruth\"], axis=1)\n",
    "Y = train_set[\"GroundTruth\"] == train_set[\"Answer.option\"]\n",
    "\n",
    "# Basic K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate different metrics\n",
    "precision_scores = cross_val_score(model, X, Y, cv=kfold, scoring='precision')\n",
    "recall_scores = cross_val_score(model, X, Y, cv=kfold, scoring='recall')\n",
    "f1_scores = cross_val_score(model, X, Y, cv=kfold, scoring='f1')\n",
    "\n",
    "print(\"K-Fold Cross Validation Results:\")\n",
    "print(f\"Precision Scores: {precision_scores}\")\n",
    "print(f\"Average Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std() * 2:.4f})\")\n",
    "print(f\"\\nRecall Scores: {recall_scores}\")\n",
    "print(f\"Average Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std() * 2:.4f})\")\n",
    "print(f\"\\nF1 Scores: {f1_scores}\")\n",
    "print(f\"Average F1: {f1_scores.mean():.4f} (+/- {f1_scores.std() * 2:.4f})\")"
   ],
   "id": "68160ccc310bed99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# create bins\n",
    "complexity_bins = np.linspace(train_set['Flesch_reading_ease'].min(),\n",
    "                              train_set['Flesch_reading_ease'].max(),\n",
    "                              11)\n",
    "\n",
    "complexity_groups = train_set.groupby(pd.cut(train_set['Flesch_reading_ease'], bins=complexity_bins))\n",
    "complexity_correct = complexity_groups.apply(lambda x: (x['GroundTruth'] == x[\"Answer.option\"]).mean() * 100)\n",
    "\n",
    "# create plot\n",
    "bars = ax.bar(range(len(complexity_correct)), complexity_correct, color='lightgreen')\n",
    "ax.set_title('Distribution of Correct Labels by Reading Complexity', pad=20)\n",
    "ax.set_xlabel('Flesch Reading Ease Score', labelpad=10)\n",
    "ax.set_ylabel('Percentage Correct (%)', labelpad=10)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(len(complexity_correct)))\n",
    "ax.set_xticklabels([f'{bin.left:.1f}-{bin.right:.1f}'\n",
    "                    for bin in complexity_correct.index],\n",
    "                   rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 1,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8cf4385bdccb1b26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# create bins\n",
    "length_bins = np.linspace(train_set['Answer.explanation_length'].min(),\n",
    "                          train_set['Answer.explanation_length'].max(),\n",
    "                          11)\n",
    "\n",
    "length_groups = train_set.groupby(pd.cut(train_set['Answer.explanation_length'], bins=length_bins))\n",
    "length_correct = length_groups.apply(lambda x: (x['GroundTruth'] == x[\"Answer.option\"]).mean() * 100)\n",
    "\n",
    "# create plot\n",
    "bars = ax.bar(range(len(length_correct)), length_correct, color='skyblue')\n",
    "ax.set_title('Distribution of Correct Labels by Explanation Length', pad=20)\n",
    "ax.set_xlabel('Explanation Length (characters)', labelpad=10)\n",
    "ax.set_ylabel('Percentage Correct (%)', labelpad=10)\n",
    "\n",
    "# add labels\n",
    "ax.set_xticks(range(len(length_correct)))\n",
    "ax.set_xticklabels([f'{int(bin.left)}-{int(bin.right)}'\n",
    "                    for bin in length_correct.index],\n",
    "                   rotation=45)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 1,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7ae4210c22f8cb66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "xgboost.plot_importance(xgb_model, max_num_features=10)",
   "id": "2c6aee779361d9e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from xgboost import plot_tree\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "plot_tree(xgb_model, num_trees=1, ax=ax)\n",
    "plt.show()"
   ],
   "id": "ef47c72ca58dc4f9"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
