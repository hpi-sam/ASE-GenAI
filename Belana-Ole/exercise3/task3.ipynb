{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Diverse Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Minutes are set to -15; which is less then 0 and it throws illegal arg exception', 'The code never gets that far. The problem is at line 279 which prevents a negative minutes value being accepted even though the programmer comments indicate that since version 2.3 negative minutes up to -59 are acceptable. The @throws IllegalArgumentException comment is also referring to versions before 2.3.', 'In the code there is a check that 0 <= minutes < 60 and the minutesOffset is -15 which does not fall into these prarmeters thus throwing an Exception', 'There is a logical check for if minuteOffset is less than 0 or greater than 59 causing it to throw an exception because the value is out of bounds (negative number)', 'YES. The issue is on line 279 (as I explained in my first question; of which I misunderstood that I was only being asked about the specific issue; not generalized issue). On line 279 the variable \"minutesOffSet\" is parameterized to throw an exception if it is < 0 or > 59. Line 279 should read \"if (minutesOffset < -59 || minutesOffset > 59) {\" because now the method can take in the number of minutes as a negative and will allow the method to properly progress to invoke/call further methods such as those asked about in the two previous questions.', 'The variable \"minutesOffset\" is checked incorrectly by the IF statement on line 279. Any negative value for \"minutesOffset\" will throw this exception; while the documentation states that \"minutesOffset\" can be negative in some cases.', \"Again the issue remains that the parameter check for minutes at line 279 doesn't allow negative numbers  and thus throws the exception.  This needs to be changed to allow up to -59 (but only if hour is positive) if the comments are up to date ... plus possibly later code as well; but certainly that for starters. \", 'This variable contains a value of -15 as set by DateTimeZone.forOffsetHoursMinutes(-2; -15). Line 279 checks to see if is a valid value; meaning that is between 0 and 59. Since it is not; an exception error is thrown in line 280.', 'Yes; the variable gets set to -15 through the arguments above. The code specifically encounters the error on line 279 when it tests if minutesOffset < 0; (-15) which is the case; so it throws the error on line 280 : Minutes out of range: with the value provided for that argument -15.', 'As noted in the comments; valid input for minutes must be in the rage -59 to +59 but on line 279 of the source minutesOffset is checked for < 0. Instead it should be minutesOFfset < -59 . Also noted in comments is that versions before 2.3 minutes had to be zero or positive. \"Minutes out of range: + minutesOffset\" is our error.', 'the variable should be defined as \"unsigned int\" if we expect it to be always positive', 'minutesOffset it is the offset in minutes from UTC; must be between 0 and 59 inclusive ', 'this is also an int; which is a signed value.', 'The second argument should be just 15', 'The value of minutes offset does not have valid argument as a result this method will not be called as and argument exception will be displayed.', \"yep; they are checking if minutesOffset < 0 to throw an exception; and as -15 <0; it gets thrown. looks like they updated the comments but not the code. and this is why comments are evil liars that can't be trusted!\", 'The error is stemming from line 279 because the value of -15 for minutesOffset is < 0. The line should be     if (minutesOffset < -59 || minutesOffset > 59) {', 'The definition seems fine to me. ', 'There should be no issue with the variable minutesOffset; as it was declared properly and should not be related to the failure when it was being used in the correct manner. minutesOffset is of type int; but it is not being miscalled anywhere in the source code.', 'I believe the issue is with hoursOffset not minutes', 'The minute variable was negative therefore; it threw the exception because it only takes numbers between 0 and 59 for minutes.', 'the conditional clause throws an error if the value of minutes is smaller than zero', 'The argument -15 is less than 0; which causes the if statement conditional on line 279 to pass. This results in the exception on line 280 being thrown. According to the comment block above the method; minutesOffset should be checked for below -59 or above 59.', 'There may be an issue as it involves not indigenous java', 'According to the comments; the minute value should be between -59 and +59; but the conditional statement is checking for a value between 0 and 59.', 'There is not an issue with this portion of the code; in fact this is where the exception we receive is thrown.  Therefore the issue probably occurs before we reach this part of the code; such as when the arguments are passed into the method.', 'Yes; this line is exactly the one that produces the exception when minutesOffset is <0. As minutesOffset; being the second argument in the function; gets the value -15 in the call to DateTimeZone.forOffsetHoursMinutes(-2; -15)', 'You are passing it a negative offset value (-15) and the conditionals are set to reject any offset that is less than 0 or greater than 59 and throw a new exception.', 'the code is incomplete. it properly checks for greater than 59 but neglects to take into account if the hours are negative before rejecting minutes for being negative. it would be more correct though not completely to check for less than -59 instead of less than 0', 'It will return the offset', 'am not good in thus questanaire', 'Line 279 written as \"minutesOffset < 0\" makes it clear it\\'s the one throwing the Exception; as the -15 in the minutes spot is clearly less than 0.', 'this cause assumes all negative minutes are bad. from the comment; negative minutes are ok when the hours are negative too. the comments specifically say its bad when the mins are negative but the hours are positive. there is a line break in the middle of that part of the comment which could lead a programmer to miss half of the info.', 'Yes ; this conditional clause is exactly the place from where the exception is thrown ; because it is not in valid range of \"minutes\"', \"It's because of second line negative value.\", 'This conditional will reject any negative minute input; even if the hour input is also negative.', 'This is the argument exception thrown.', '-15 is less then 0; so it throws IllegalArgumentException', 'Value passed in minutes -15;where as it checks (<0 | >53).', 'DateTimeZone.forOffsetHoursMinutes(-2; -15) is an invalid argument so it will directly throw an exception. Hence there is no issue between the conditional statement.']\n"
     ]
    }
   ],
   "source": [
    "# Load bug reports explanations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"../../data/answerList_data.csv\")\n",
    "\n",
    "bug_reports_data = {}\n",
    "\n",
    "# read one file per failing method\n",
    "for failing_method in data['FailingMethod'].unique():\n",
    "    with open(Path(f\"../exercise2/{failing_method}.txt\"), 'r') as f:\n",
    "        # individual explanations are separated by a newline\n",
    "        explanations = f.read().split(\"\\n\")\n",
    "        explanations = [explanation for explanation in explanations if explanation != \"\"]\n",
    "        bug_reports_data[failing_method] = explanations   \n",
    "\n",
    "print(bug_reports_data[\"HIT01_8\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ground truth explanations\n",
    "with open(\"ground_truth_explanations.json\", \"r\") as f:\n",
    "    ground_truth_explanations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT03_6: 64.75\n",
      "HIT05_35: 58.62\n",
      "HIT07_33: 53.04\n",
      "HIT02_24: 51.18\n",
      "HIT04_7: 50.16\n",
      "HIT08_54: 46.81\n",
      "HIT06_51: 43.02\n",
      "HIT01_8: 28.67\n",
      "HIT01_8: 22.936\n",
      "HIT02_24: 40.944\n",
      "HIT03_6: 51.8\n",
      "HIT04_7: 40.128\n",
      "HIT05_35: 46.896\n",
      "HIT06_51: 34.416\n",
      "HIT07_33: 42.432\n",
      "HIT08_54: 37.448\n"
     ]
    }
   ],
   "source": [
    "# calculate readability scores for the ground truth explanations\n",
    "\n",
    "import textstat\n",
    "readability_scores = {}\n",
    "for method, explanation in ground_truth_explanations.items():\n",
    "    readability_scores[method] = textstat.flesch_reading_ease(explanation)\n",
    "\n",
    "# sort the explanations by readability\n",
    "sorted_explanations = sorted(readability_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print readibility scores\n",
    "for method, score in sorted_explanations:\n",
    "    print(f\"{method}: {score}\")\n",
    "\n",
    "readability_threshold = {}\n",
    "for method, score in readability_scores.items():\n",
    "    readability_threshold[method] = round(score * 0.8,3)\n",
    "\n",
    "# print readability thresholds\n",
    "for method, score in readability_threshold.items():\n",
    "    print(f\"{method}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bzwad\\anaconda3\\envs\\DSWebscrape\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define simialrity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Load a high-performance model for semantic similarity\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def compute_similarity(ground_truth, user_explanation):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between the ground truth explanation and a user-given explanation.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode([ground_truth, user_explanation], convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT01_8: 0.5436896428233012\n",
      "HIT02_24: 0.3701791672501713\n",
      "HIT03_6: 0.25586818529409355\n",
      "HIT04_7: 0.2914702493418008\n",
      "HIT05_35: 0.42632644800469277\n",
      "HIT06_51: 0.30135327534129225\n",
      "HIT07_33: 0.4918408831271032\n",
      "HIT08_54: 0.3938455417131384\n",
      "HIT01_8: 0.7063461601734161\n",
      "HIT02_24: 0.5649046778678893\n",
      "HIT03_6: 0.32944928407669066\n",
      "HIT04_7: 0.38390407562255857\n",
      "HIT05_35: 0.5162835419178009\n",
      "HIT06_51: 0.4160767793655395\n",
      "HIT07_33: 0.6244548857212067\n",
      "HIT08_54: 0.4770480155944824\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity for all user explanations and set threshold\n",
    "similarity_scores = {}\n",
    "\n",
    "for method, ground_truth in ground_truth_explanations.items():\n",
    "    scores = [compute_similarity(ground_truth, user_exp) for user_exp in bug_reports_data.get(method, [])]\n",
    "    similarity_scores[method] = scores\n",
    "\n",
    "# average similarity per failing method\n",
    "average_similarity_scores = {}\n",
    "for method, scores in similarity_scores.items():\n",
    "    average_similarity_scores[method] = sum(scores) / len(scores)\n",
    "\n",
    "for method, score in average_similarity_scores.items():\n",
    "    print(f\"{method}: {score}\")\n",
    "\n",
    "# set similarity threshold to 70th percentile\n",
    "similarity_threshold = {}\n",
    "for method, scores in similarity_scores.items():\n",
    "    similarity_threshold[method] = np.percentile(scores, 70)\n",
    "\n",
    "for method, score in similarity_threshold.items():\n",
    "    print(f\"{method}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate summaries\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "#load api key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "# Function to generate summaries using LLaMA 3.2\n",
    "def generate_summary(method, explanations):\n",
    "    combined_text = \" \".join(explanations)\n",
    "\n",
    "    # Truncate input to fit within token limits\n",
    "    max_input_length = 4000\n",
    "    combined_text = combined_text[:max_input_length]\n",
    "\n",
    "    # OpenRouter API call\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-3.2-1b-instruct:free\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert summarizer. Given detailed bug reports, provide a single, comprehensive explanation that includes all necessary and sufficient information to understand and fix the bug.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize the following bug reports:\\n\\n{combined_text}\",\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=200  # Adjust to control summary length\n",
    "    )\n",
    "\n",
    "    if completion.choices is not None:\n",
    "        return completion.choices[0].message.content\n",
    "    else:\n",
    "        return \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define utility functions\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import enchant\n",
    "\n",
    "def contains_english_word(text):\n",
    "    \"\"\"\n",
    "    Check if a string contains at least one English word.\n",
    "    Returns True if an English word is found, False otherwise.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input string to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if contains English word, False otherwise\n",
    "    \"\"\"\n",
    "    # Initialize English dictionary\n",
    "    dictionary = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    # Clean the string - keep only letters and spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Split into potential words\n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "    # Check each potential word\n",
    "    return any(dictionary.check(word.lower()) for word in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity of Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Q 3.1) There are many different ways which we could use to measure the diversity of explanations. We could, for example, use the type-token ratio or the number of unique words in the explanations to measure the diversity of words used in the explanations. Since we did not get great results with the TTR in previous experiments, we will not use this approach. Instead, we want to regard the Shannon Entropy of the explanations. The Shannon Entropy is a measure of the uncertainty in a random variable. In our case, the random variable is the choice of words in the explanations. The Shannon Entropy is defined as follows:\n",
    "\n",
    "$$H(X) = - \\sum_{i=1}^{n} p(x_i) \\cdot \\log_2(p(x_i))$$\n",
    "\n",
    "where $p(x_i)$ is the probability of the $i$-th word in the explanation. Shannon entropy measures the randomness or uncertainty in a distributionâ€”in this case, the distribution of words in a text. Higher entropy means more unpredictability, while lower entropy suggests redundancy or repetition. The minimum entropy is 0, which occurs when all words are the same. The maximum entropy is the logarithm of the number of words in the text, which occurs when all words are equally likely. For normal written English, entropy usually ranges between 4 and 8 bits per word, depending on vocabulary richness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also measure diversity by looking at the semantic diversity of explanations using the embedding distance (mean pairwise cosine similarity) between the explanations. Since this directly contradicts a high similarity between explanations, we decided against this approach.\n",
    "In the broader range of diversity, we could look at more features than the explanation itself, e.g. at associated categorical features like the bug reporters demographic data, their experience level or the country that they are from. However, this makes it harder to define exact thresholds for diversity.\n",
    "\n",
    "As the Shannon Entropy seems like the most straight forward approach, we will use this to measure the diversity of explanations in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def shannon_entropy(text):\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    total_words = len(words)\n",
    "    entropy = -sum((count/total_words) * math.log2(count/total_words) for count in word_counts.values())\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT08_54: 5.358714497742255\n",
      "HIT07_33: 5.25585347326784\n",
      "HIT06_51: 5.115114023681427\n",
      "HIT01_8: 5.10341455748809\n",
      "HIT03_6: 4.999664476749764\n",
      "HIT05_35: 4.898153434632013\n",
      "HIT02_24: 4.8219280948873635\n",
      "HIT04_7: 4.812209613812088\n"
     ]
    }
   ],
   "source": [
    "# calculate shannon entropy for ground truth explanations\n",
    "ground_truth_entropy = {}\n",
    "for method, explanation in ground_truth_explanations.items():\n",
    "    ground_truth_entropy[method] = shannon_entropy(explanation)\n",
    "\n",
    "# sort the explanations by entropy\n",
    "sorted_entropy = sorted(ground_truth_entropy.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print entropy scores\n",
    "for method, score in sorted_entropy:\n",
    "    print(f\"{method}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all our ground truth explanations achieve a Shannon Entropy of 4-6 bits per word. We will use this as a reference point for the diversity of explanations. This moderate Entropy is in general desirable, as it indicates a good balance between clarity and lexical variety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max readability and max similarity values independent of the diversity per bug report are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT01_8: 0.820572555065155, 99.23\n",
      "HIT02_24: 0.7392517328262329, 121.22\n",
      "HIT03_6: 0.6439655423164368, 121.22\n",
      "HIT04_7: 0.7673725485801697, 120.21\n",
      "HIT05_35: 0.7223565578460693, 104.64\n",
      "HIT06_51: 0.6698489785194397, 119.19\n",
      "HIT07_33: 0.7574557662010193, 118.18\n",
      "HIT08_54: 0.7468466758728027, 120.21\n"
     ]
    }
   ],
   "source": [
    "# Process all methods in a single pass to find highest similarity and readability scores/explanations\n",
    "highest_similarity_scores = {}\n",
    "highest_readability_scores = {}\n",
    "most_similar_explanations = {}\n",
    "most_readable_explanations = {}\n",
    "\n",
    "for method, explanations in bug_reports_data.items():\n",
    "    # Calculate similarity scores for all explanations\n",
    "    similarity_scores = [compute_similarity(ground_truth_explanations[method], explanation) for explanation in explanations]\n",
    "    max_similarity_index = similarity_scores.index(max(similarity_scores))\n",
    "    \n",
    "    # Store highest similarity score and corresponding explanation\n",
    "    highest_similarity_scores[method] = similarity_scores[max_similarity_index]\n",
    "    most_similar_explanations[method] = explanations[max_similarity_index]\n",
    "    \n",
    "    # Calculate readability scores for all explanations\n",
    "    readability_scores = []\n",
    "    for explanation in explanations:\n",
    "        if len(explanation.split()) == 0 or not contains_english_word(explanation):\n",
    "            readability_scores.append(0)\n",
    "        else:\n",
    "            readability_scores.append(textstat.flesch_reading_ease(explanation))\n",
    "    \n",
    "    max_readability_index = readability_scores.index(max(readability_scores))\n",
    "    \n",
    "    # Store highest readability score and corresponding explanation\n",
    "    highest_readability_scores[method] = readability_scores[max_readability_index]\n",
    "    most_readable_explanations[method] = explanations[max_readability_index]\n",
    "\n",
    "# Print scores\n",
    "for method, (similarity, readability) in zip(highest_similarity_scores.keys(), \n",
    "                                           zip(highest_similarity_scores.values(), \n",
    "                                               highest_readability_scores.values())):\n",
    "    print(f\"{method}: {similarity}, {readability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The readability scores are quite high because some people put very simple \"explanations\" like \"yes\" or \"no\" as their answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the high readability and high similarity explanations and \n",
    "# see if they are also high entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSWebscrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
