{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Task 3",
   "id": "1d10a0f60c24f48d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question 3.1 - How would you measure diversity?\n",
    "\n",
    "First we look into the question, how we can measure the diversity of a subset of different answers. To calculate the diversity, we need to differentiate two types of answers: _categorical_ and _continuous_.\n",
    "\n",
    "An example for a categorical column is gender. We can measure the diversity in this case with the _entropy_ measure. Since entropy measures how spread out or balanced the selected values are, it is ideal. In the end we want to normalize the entropy to account for classes that are not in the selected subset.\n",
    "\n",
    "For continuous data columns like _age_ we can measure the diversity using the _standard deviation_. We could also use the _entropy_ and treat every age as a separate class. However, this would mean that we don't account for the absolute age difference between workers, just for different ages. We therefore decided that the standard deviation is the better metric.\n",
    "To get a score in-between 0 and 1, it has to be normalized: The std. deviation of the subset is divided by the std. deviation of the maximum possible std. deviation of the full dataset. This is done by calculating the difference of the maximum and minimum values of the original dataset and then dividing it by two.\n",
    "\n",
    "In the end we calculate our total diversity score by calculating the diversity of each relevant column and then generating their mean."
   ],
   "id": "7502b212d626478a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n"
   ],
   "id": "5c37047fb2817d53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Excerpt of the original dataframe:",
   "id": "4ef869c486701f70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('../../data/answerList_data.csv')\n",
    "df['correctness'] = (((df['GroundTruth'] == 0.0) & (df['Answer.option'] == 'NO')) | (\n",
    "        (df['GroundTruth'] == 1.0) & (df['Answer.option'] == 'YES'))).astype(int)\n",
    "\n",
    "df"
   ],
   "id": "5e8676ca27b5d2f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For our analysis we pick the Failing Method \"HIT01_8\" and look at question no. 1.\n",
    "We then filter out incorrect answers and are left with 12 correct answers, given to question 1."
   ],
   "id": "bd9314629c06abb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_question_1 = df[df['Question.ID'] == 1]\n",
    "df_question_1_correct = df_question_1[df_question_1['correctness'] == 1]\n",
    "\n",
    "df_question_1_correct"
   ],
   "id": "6004b5723205083a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For demonstration we can print the gender diversity and its normalized value of all picked answers compared to the full dataset:\n",
    "\n",
    "```python\n",
    "def categorical_diversity(series):\n",
    "    counts = series.value_counts(normalize=True)\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "\n",
    "def categorical_diversity_norm(series, series_full):\n",
    "    k = series_full.nunique()\n",
    "    return categorical_diversity(series) / (np.log2(k) if k > 1 else 1)\n",
    "```"
   ],
   "id": "ca04c5ad2c6ca0cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def categorical_diversity(series):\n",
    "    counts = series.value_counts(normalize=True)\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "\n",
    "def categorical_diversity_norm(series, series_full):\n",
    "    k = series_full.nunique()\n",
    "    return categorical_diversity(series) / (np.log2(k) if k > 1 else 1)\n",
    "\n",
    "\n",
    "gender_diversity = categorical_diversity(df_question_1_correct['Worker.gender'])\n",
    "gender_diversity_norm = categorical_diversity_norm(df_question_1_correct['Worker.gender'], df['Worker.gender'])\n",
    "\n",
    "print('Gender diversity of all answers:', gender_diversity)\n",
    "print('Normalized gender diversity', gender_diversity_norm)"
   ],
   "id": "359486ca454c4667"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For continuous variables, we use the following method:\n",
    "```python\n",
    "def coefficient_variation(series, series_full):\n",
    "    if len(series) < 2:\n",
    "        return 0\n",
    "    return series.std() / ((series_full.max() - series_full.min()) / 2)\n",
    "````\n",
    "\n",
    "For demonstration purposes we print the age diversity of all picked answers compared to the full dataset:"
   ],
   "id": "f559284491ea30b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def coefficient_variation(series, series_full):\n",
    "    if len(series) < 2:\n",
    "        return 0\n",
    "    return series.std() / ((series_full.max() - series_full.min()) / 2)\n",
    "\n",
    "age_diversity = coefficient_variation(df_question_1_correct['Worker.age'], df['Worker.age'])\n",
    "\n",
    "print('Normalized age diversity', age_diversity)"
   ],
   "id": "bd5d824f2728e529"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def combined_diversity_score(df_selection, df_all, categorical_cols, continuous_cols):\n",
    "    total_number_cols = len(categorical_cols) + len(continuous_cols)\n",
    "\n",
    "\n",
    "    categorical_score = sum(1 / total_number_cols * categorical_diversity_norm(df_selection[col], df_all[col]) for col in categorical_cols)\n",
    "    continuous_score = sum(1 / total_number_cols * coefficient_variation(df_selection[col], df_all[col]) for col in continuous_cols)\n",
    "\n",
    "    return categorical_score + continuous_score\n",
    "\n",
    "def list_diversity_scores(df_selection, df_all, categorical_cols, continuous_cols):\n",
    "    total_number_cols = len(categorical_cols) + len(continuous_cols)\n",
    "\n",
    "    categorical_scores = [1 / total_number_cols * categorical_diversity_norm(df_selection[col], df_all[col]) for col in categorical_cols]\n",
    "    continuous_scores = [1 / total_number_cols * coefficient_variation(df_selection[col], df_all[col]) for col in continuous_cols]\n",
    "\n",
    "    return categorical_scores + continuous_scores"
   ],
   "id": "166a973038ba1c6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For our analysis, we chose the following categorical columns\n",
    "- Worker.profession\n",
    "- Worker.gender\n",
    "\n",
    "and the following continuous columns\n",
    "- Worker.age\n",
    "- Worker.yearsOfExperience"
   ],
   "id": "2d40735d642770ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To combine multiple factors into a single diversity score, we take both categorical and continous columns, give each an equal weight and sum them up.\n",
    "The diversity score is then normalized by the number of factors.\n",
    "\n",
    "The combined diversity score of our picked answers compared to the full dataset:"
   ],
   "id": "3a35f8bc1d7d627e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "categorical_cols = ['Worker.profession', 'Worker.gender']\n",
    "continuous_cols = ['Worker.yearsOfExperience', 'Worker.age']\n",
    "\n",
    "diversity_score = combined_diversity_score(df_question_1_correct, df, categorical_cols, continuous_cols)\n",
    "\n",
    "print('Combined Diversity', diversity_score)"
   ],
   "id": "feb04bbd2316b219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def select_n(df, n):\n",
    "    return df.sample(n=n)"
   ],
   "id": "580f62057c84dba0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for n in range(1, 10):\n",
    "    sample_df = select_n(df_question_1_correct, n)\n",
    "    div_scores = list_diversity_scores(sample_df, df_question_1_correct, categorical_cols, continuous_cols)\n",
    "    print('Sample df\\n', sample_df[categorical_cols+continuous_cols])\n",
    "    print('Diversity score\\n', div_scores)\n",
    "    print('Total diversity score', combined_diversity_score(sample_df, df_question_1_correct, categorical_cols, continuous_cols))\n",
    "    print('\\n\\n')\n"
   ],
   "id": "86e8fe16bfd32dd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Subsets\n",
    "\n",
    "We are now going to look at subsets of the 12 previously picked answers.\n",
    "We will evaluate these subset according to three different measures:\n",
    "- diversity (regarding in regard to persons)\n",
    "- readability\n",
    "- semantic similarity of the subset to our hand-crafted ground truth\n"
   ],
   "id": "80acb107d7141a89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "ground_truth = \"The IF statement in line 279 checks whether minutesOffset is set to a value between 0 and 59. If not, an IllegalArgumentException is thrown. This is a bug because the minutesOffset may also be negative. The IF statement should check for the minutesOffset to be between -59 and 59.\"\n",
    "\n",
    "bug_group = df_question_1_correct"
   ],
   "id": "266db4c77d8c666f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To check for similarity, we embed the two sentences to be compared with the model\n",
    "\"all-MiniLM-L6-v2\" and use cosine similarity on the transformed embeddings.\n",
    "```python\n",
    "def calculate_cosine_similarity(hyp, ref):\n",
    "    hyp_embedding = model.encode(hyp)\n",
    "    ref_embedding = model.encode(ref)\n",
    "    return np.dot(hyp_embedding, ref_embedding) / (np.linalg.norm(hyp_embedding) * np.linalg.norm(ref_embedding))\n",
    "\n",
    "```"
   ],
   "id": "e6529825dd35b196"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_cosine_similarity(hyp, ref, ref_embedding=None):\n",
    "    hyp_embedding = model.encode(hyp)\n",
    "    if ref_embedding is None:\n",
    "        ref_embedding = model.encode(ref)\n",
    "    return np.dot(hyp_embedding, ref_embedding) / (np.linalg.norm(hyp_embedding) * np.linalg.norm(ref_embedding))\n"
   ],
   "id": "6367f09e4909d0da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To calculate a readability score, we are using the Flesch Reading Ease formula.",
   "id": "c2b0bf23e6216603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import textstat\n",
    "def calculate_flesch_reading_ease(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "def calculate_automated_readability_index(text):\n",
    "    return textstat.automated_readability_index(text)"
   ],
   "id": "5178717234c5c77e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def coefficient_variation(series, series_full):\n",
    "    if len(series) < 2:\n",
    "        return 0\n",
    "    return series.std() / ((series_full.max() - series_full.min()) / 2)\n",
    "\n",
    "def categorical_diversity(series):\n",
    "    counts = series.value_counts(normalize=True)\n",
    "    return entropy(counts, base=2)\n",
    "\n",
    "\n",
    "def categorical_diversity_norm(series, series_full):\n",
    "    k = series_full.nunique()\n",
    "    return categorical_diversity(series) / (np.log2(k) if k > 1 else 1)\n",
    "\n",
    "def combined_diversity_score(df_selection, df_all, categorical_cols, continuous_cols):\n",
    "    total_number_cols = len(categorical_cols) + len(continuous_cols)\n",
    "\n",
    "\n",
    "    categorical_score = sum(1 / total_number_cols * categorical_diversity_norm(df_selection[col], df_all[col]) for col in categorical_cols)\n",
    "    continuous_score = sum(1 / total_number_cols * coefficient_variation(df_selection[col], df_all[col]) for col in continuous_cols)\n",
    "\n",
    "    return categorical_score + continuous_score"
   ],
   "id": "bd89814277b0af03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the full dataset of answers is relatively small (12), we can search through all possible subsets of lengths 1 to 12 and compare their scores in all three categories. Below are the number of combinations to check for a subset of size `n`",
   "id": "f118855112fc1d5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use itertools to get all possible subsets of bug_group rows\n",
    "import itertools\n",
    "\n",
    "def get_subsets(df):\n",
    "    for i in range(1, len(df) + 1):\n",
    "        amount = 0\n",
    "        for subset in itertools.combinations(df.iterrows(), i):\n",
    "            amount += 1;\n",
    "            yield df.loc[[x[0] for x in subset]]\n",
    "\n",
    "all_subsets = list(get_subsets(bug_group))"
   ],
   "id": "8b4a2074eaca0345"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "categorical_cols = ['Worker.profession', 'Worker.gender']\n",
    "continuous_cols = ['Worker.yearsOfExperience', 'Worker.age']"
   ],
   "id": "7369c7c3595d1b16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below is an excerpt of the three scores for each possible subset. The text-content to be scored per subset is defined by the concatenation of the explanations of the subset. Keep in mind, that no permutations are included. The order is random.",
   "id": "e63037d4001d4d38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ground_truth_embedding = model.encode(ground_truth)\n",
    "\n",
    "results = []\n",
    "for subset in tqdm(all_subsets):\n",
    "    # concat all strings of Answer.explanation in subset\n",
    "    subset_text = subset['Answer.explanation'].str.cat(sep='\\n\\n')\n",
    "    readability_score = calculate_flesch_reading_ease(subset_text)\n",
    "    similarity_score = calculate_cosine_similarity(subset_text, ground_truth, ref_embedding=ground_truth_embedding)\n",
    "    diversity_score = combined_diversity_score(subset, bug_group, categorical_cols, continuous_cols)\n",
    "    results.append((readability_score, similarity_score, diversity_score))\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['readability', 'similarity', 'diversity'])\n",
    "df_results"
   ],
   "id": "5de8e3466c52a7bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_column(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "df_results['readability.norm'] = normalize_column(df_results['readability'])\n",
    "df_results['similarity.norm'] = normalize_column(df_results['similarity'])\n",
    "df_results['diversity.norm'] = normalize_column(df_results['diversity'])"
   ],
   "id": "b8a5d370f6a175dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# add normed scores\n",
    "df_results['score'] = (df_results['readability.norm'] + df_results['similarity.norm'] + df_results['diversity.norm']) / 3\n",
    "\n",
    "# sort by score descending\n",
    "df_results = df_results.sort_values('score', ascending=False)\n"
   ],
   "id": "1db70600450ecbc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @hidden_cell\n",
    "def print_subset(i): # i is the best ranked index\n",
    "    score_row = df_results.iloc[i]\n",
    "    original_index = df_results.index[i]\n",
    "    subset = all_subsets[original_index]\n",
    "\n",
    "    print('Readability:', score_row['readability.norm'])\n",
    "    print('Similarity:', score_row['similarity.norm'])\n",
    "    print('Diversity:', score_row['diversity.norm'])\n",
    "    print('Score:', score_row['score'])\n",
    "    # print the concatted Answer.explanation\n",
    "    print(subset['Answer.explanation'].str.cat(sep='\\n\\n'))"
   ],
   "id": "4406c7b0a59ab6ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @hidden_cell\n",
    "print_subset(0)\n",
    "print_subset(1)"
   ],
   "id": "603ffd3293c34387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @hidden_cell\n",
    "\n",
    "# get the best subset's index\n",
    "best_subset_index = df_results.index[0]\n",
    "best_subset_index\n",
    "\n",
    "best_subset = all_subsets[best_subset_index]\n",
    "best_subset"
   ],
   "id": "3e4a60a6379a4c15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To understand which scores can be maximized while taking loss of other scores into account, we can use pareto fronts.\n",
    "\n",
    "At first, we can remove any dominated solutions from the dataframe. A solution is dominated if there is another solution that is better in all scores."
   ],
   "id": "4cf0b3e4555038f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def is_pareto_efficient(costs):\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    display(len(is_efficient))\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient] > c, axis=1)\n",
    "            is_efficient[i] = True\n",
    "    return is_efficient\n",
    "\n",
    "pareto_mask = is_pareto_efficient(df_results[[\"readability.norm\", \"similarity.norm\", \"diversity.norm\"]].values)\n",
    "pareto_df = df_results[pareto_mask]\n",
    "pareto_df"
   ],
   "id": "fd8918e9a0122944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We reduced the number of interesting subsets from 4095 to 83.\n",
    "\n",
    "Ordered by the total score we see that the best score is 0.876. Printed is the best possible concatenated explanation, regarding all three scores:"
   ],
   "id": "177f414ab232e38d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(best_subset['Answer.explanation'].str.cat(sep='\\n\\n'))",
   "id": "97031576c586d921"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For reference, here ist the chosen ground truth from Task 2:\n",
    "\n",
    ">\"The IF statement in line 279 checks whether minutesOffset is set to a value between 0 and 59. If not, an IllegalArgumentException is thrown. This is a bug because the minutesOffset may also be negative. The IF statement should check for the minutesOffset to be between -59 and 59.\"\""
   ],
   "id": "325633f1b48d26e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib\n",
    "matplotlib.use('MacOSX')\n"
   ],
   "id": "8f4b9fd3714dc98f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_values = pareto_df[\"readability.norm\"] + pareto_df[\"similarity.norm\"] + pareto_df[\"diversity.norm\"]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(pareto_df[\"readability.norm\"],\n",
    "           pareto_df[\"similarity.norm\"],\n",
    "           pareto_df[\"diversity.norm\"],\n",
    "           c=color_values,\n",
    "           cmap='plasma',\n",
    "           label='Pareto-optimal points')\n",
    "\n",
    "ax.set_xlabel('Readability')\n",
    "ax.set_ylabel('Similarity')\n",
    "ax.set_zlabel('Diversity')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f6d3bfbfa0f9d29a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| ![image-4](attachment:image-4.png) | ![image](attachment:image.png) |\n",
    "|---------------------------|------------------------|\n",
    "| ![image-2](attachment:image-2.png) | ![image-3](attachment:image-3.png) |\n"
   ],
   "id": "b7c52d7ec48a21b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_values = pareto_df[\"readability.norm\"] + pareto_df[\"similarity.norm\"] + pareto_df[\"diversity.norm\"]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(pareto_df[\"readability.norm\"],\n",
    "           pareto_df[\"similarity.norm\"],\n",
    "           pareto_df[\"diversity.norm\"],\n",
    "           c=color_values,\n",
    "           cmap='plasma',\n",
    "           label='Pareto-optimal points')\n",
    "\n",
    "ax.set_xlabel('Readability')\n",
    "ax.set_ylabel('Similarity')\n",
    "ax.set_zlabel('Diversity')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "231b96828cb11b9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "# Create a figure with 3 subplots for 2D projections\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "scatters = []\n",
    "\n",
    "# (1) Similarity vs Diversity\n",
    "sc = axes[0].scatter(pareto_df[\"similarity.norm\"], pareto_df[\"diversity.norm\"], c=color_values, cmap='plasma', edgecolors='k')\n",
    "axes[0].set_xlabel('Similarity')\n",
    "axes[0].set_ylabel('Diversity')\n",
    "axes[0].set_title('Projection: Similarity vs Diversity')\n",
    "scatters.append(sc)\n",
    "\n",
    "# (2) Similarity vs Readability\n",
    "sc = axes[1].scatter(pareto_df[\"similarity.norm\"], pareto_df[\"readability.norm\"], c=color_values, cmap='plasma', edgecolors='k')\n",
    "axes[1].set_xlabel('Similarity')\n",
    "axes[1].set_ylabel('Readability')\n",
    "axes[1].set_title('Projection: Similarity vs Readability')\n",
    "scatters.append(sc)\n",
    "\n",
    "# (3) Readability vs Diversity\n",
    "sc = axes[2].scatter(pareto_df[\"readability.norm\"], pareto_df[\"diversity.norm\"], c=color_values, cmap='plasma', edgecolors='k')\n",
    "axes[2].set_xlabel('Readability')\n",
    "axes[2].set_ylabel('Diversity')\n",
    "axes[2].set_title('Projection: Readability vs Diversity')\n",
    "scatters.append(sc)\n",
    "\n",
    "cbar = fig.colorbar(scatters[0], ax=axes, orientation='vertical', fraction=0.02, pad=0.05)\n",
    "cbar.set_label('Color Value (Readability + Similarity + Diversity)')\n",
    "\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.show()"
   ],
   "id": "c82633877552c2e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2\n",
    "Max Score (similarity, readability) while compromising diversity\n",
    "\n",
    "df_results['score_similarity_readability'] = (df_results['readability.norm'] + df_results['similarity.norm']) / 2\n"
   ],
   "id": "25ccaae96e042fd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_results['score_similarity_readability'] = (df_results['readability.norm'] + df_results['similarity.norm']) / 2\n",
    "\n",
    "# sort by score descending\n",
    "df_results.sort_values('score_similarity_readability', ascending=False).head(3)"
   ],
   "id": "2e39a45b1c390a28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Indeed, by sacrificing diversity we can achieve a higher score fo readability and similarity: 0.891",
   "id": "8680e8bd896abcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Highest diversity at maximum similarity (compromising on readability)",
   "id": "7cdc87f8e94ae580"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_results.sort_values('similarity.norm', ascending=False).head(5)",
   "id": "8ff2de587be1b02d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sorting by normed similarity and looking at the perfect maximum of 1.000, we get a diversity of 0.258.\n",
    "\n",
    "Compromising slightly on similarity, picking the third entry at 0.978, we can achieve a diversity score of 0.693."
   ],
   "id": "5899bf53c169af81"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
