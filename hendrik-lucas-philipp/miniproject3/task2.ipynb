{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "## Manual Ground Truth \n",
    "We pick the first question that contains a bug (Question.ID = 1) and all correct explanations to manually formulate a ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes are set to -15; which is less then 0 and it throws illegal arg exception\n",
      "In the code there is a check that 0 <= minutes < 60 and the minutesOffset is -15 which does not fall into these prarmeters thus throwing an Exception\n",
      "There is a logical check for if minuteOffset is less than 0 or greater than 59 causing it to throw an exception because the value is out of bounds (negative number)\n",
      "YES. The issue is on line 279 (as I explained in my first question; of which I misunderstood that I was only being asked about the specific issue; not generalized issue). On line 279 the variable \"minutesOffSet\" is parameterized to throw an exception if it is < 0 or > 59. Line 279 should read \"if (minutesOffset < -59 || minutesOffset > 59) {\" because now the method can take in the number of minutes as a negative and will allow the method to properly progress to invoke/call further methods such as those asked about in the two previous questions.\n",
      "The variable \"minutesOffset\" is checked incorrectly by the IF statement on line 279. Any negative value for \"minutesOffset\" will throw this exception; while the documentation states that \"minutesOffset\" can be negative in some cases.\n",
      "This variable contains a value of -15 as set by DateTimeZone.forOffsetHoursMinutes(-2; -15). Line 279 checks to see if is a valid value; meaning that is between 0 and 59. Since it is not; an exception error is thrown in line 280.\n",
      "Yes; the variable gets set to -15 through the arguments above. The code specifically encounters the error on line 279 when it tests if minutesOffset < 0; (-15) which is the case; so it throws the error on line 280 : Minutes out of range: with the value provided for that argument -15.\n",
      "As noted in the comments; valid input for minutes must be in the rage -59 to +59 but on line 279 of the source minutesOffset is checked for < 0. Instead it should be minutesOFfset < -59 . Also noted in comments is that versions before 2.3 minutes had to be zero or positive. \"Minutes out of range: + minutesOffset\" is our error.\n",
      "the variable should be defined as \"unsigned int\" if we expect it to be always positive\n",
      "The value of minutes offset does not have valid argument as a result this method will not be called as and argument exception will be displayed.\n",
      "yep; they are checking if minutesOffset < 0 to throw an exception; and as -15 <0; it gets thrown. looks like they updated the comments but not the code. and this is why comments are evil liars that can't be trusted!\n",
      "The error is stemming from line 279 because the value of -15 for minutesOffset is < 0. The line should be     if (minutesOffset < -59 || minutesOffset > 59) {\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/answerList_data.csv')\n",
    "\n",
    "df['correctness'] = (((df['GroundTruth'] == 0.0) & (df['Answer.option'] == 'NO')) | (\n",
    "            (df['GroundTruth'] == 1.0) & (df['Answer.option'] == 'YES'))).astype(int)\n",
    "\n",
    "df_correct_question1 = df[(df['correctness'] == 1) & (df['Question.ID'] == 1)]\n",
    "explanations = df_correct_question1['Answer.explanation']\n",
    "for t in explanations:    \n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our manual ground truth is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth = \"The IF statement in line 279 checks whether minutesOffset is set to a value between 0 and 59. If not, an IllegalArgumentException is thrown. This is a bug because the minutesOffset may also be negative. The IF statement should check for the minutesOffset to be between -59 and 59.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we choose metrics to evaluate readability and semantic similarity between the manual ground truth and the explanations generated by the LLM.\n",
    "\n",
    "## Readability\n",
    "\n",
    "### Flesch Reading Ease and Automated Readability Index\n",
    "We choose the Flesch Reading Ease metric and Automated Readability Index, as they are well-known readability metrics that are easy to compute and simple to interpret. The Flesch Reading Ease gives a score between 0 and 100, where a score between 60 and 80 is considered easy to read. The Automated Readability Index gives a score between 1 and 14, where for most readers an ideal score is between 7 and 9.\n",
    "\n",
    "\n",
    "## Semantic Similarity\n",
    "\n",
    "### BLEU\n",
    "We selected BLEU because of its simplicity and our prior experience with it from the previous miniproject. BLEU is widely recognized as a standard metric for measuring semantic similarity and requires minimal computational effort.\n",
    "\n",
    "### Cosine Similarity of Embeddings\n",
    "A disadvantage of BLEU is that it does not consider the semantic meaning of the explanations. Therefore, we also compute the cosine similarity of the embeddings of the explanations and the ground truth. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def calculate_bleu(hyp, ref):\n",
    "    hypothesis = hyp.split()\n",
    "    reference = ref.split()\n",
    "    return nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bddbcf4dc98446c9c5cac9537bbce12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e8c2d9bb3c402ca0e233a3b3b37d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f384980e99954e56bcb17598cb86d6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37c2690d82d48e89c88a42134199ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7869292832f04d93a4a39e0809f5f399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f99ef8bdb1a479eae0496d4fe027e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64359c4dfea74eda89263410ef28f5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a5aecec23240ddabcadf297b88877e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6683f0600cf46d4a98b7d82a1b89ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337bca2495494659b8141b1cd2324ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354fe16fbc464f1c8fd62dbfad9a0500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_cosine_similarity(hyp, ref):\n",
    "    hyp_embedding = model.encode(hyp)\n",
    "    ref_embedding = model.encode(ref)\n",
    "    return np.dot(hyp_embedding, ref_embedding) / (np.linalg.norm(hyp_embedding) * np.linalg.norm(ref_embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "def calculate_flesch_reading_ease(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "def calculate_automated_readability_index(text):\n",
    "    return textstat.automated_readability_index(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the prompt that performed better in our last mini project\n",
    "def get_prompt_2(explanations):\n",
    "        return f\"\"\"\n",
    "Please summarize these reports of the same bug. \n",
    "Remove redundant information, but make sure that every information given in the explanations is retained that would be needed to fix the bug. \n",
    "Summarize the report of the bug concisely. Keep the summary as short as possible.\n",
    "\n",
    "These are the reports:\n",
    "\n",
    "{explanations}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "def query_llm(explanations):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": get_prompt_2(str(explanations)),\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1 to n explanations, 3 times and take average\n",
    "# plot metric scores\n",
    "# find good threshold to decide how many explanations to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def sample_explanations(explanations, n):\n",
    "    return explanations.sample(n)\n",
    "\n",
    "# plot metric scores\n",
    "def plot_metric_scores(metric_scores):\n",
    "    plt.plot(metric_scores)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
